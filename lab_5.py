# -*- coding: utf-8 -*-
"""LAB 5

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gNOoEAtzhivDpwH6wcFpK6bQLiKbP8Pg

Demonstrate the usage of dropout, gradient clipping and multitask learning with early
stopping in a neural network training scenario.

**DROPOUT AND GRADIENT CLIPPING**
"""

import os
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
np.random.seed(42); tf.random.set_seed(42)

# --- Generate synthetic binary dataset ---
X, y = make_classification(n_samples=1000, n_features=20,
                           n_classes=2, random_state=42)
Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42)

# --- Model with Dropout ---
def dropout_model(drop=0.2):
    m = tf.keras.Sequential([
        tf.keras.layers.Dense(64, activation='relu', input_shape=(20,)),
        tf.keras.layers.Dropout(drop),
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dropout(drop),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return m

# --- Model with Gradient Clipping ---
def clip_model(clip_norm=1.0):
    m = tf.keras.Sequential([
        tf.keras.layers.Dense(64, activation='relu', input_shape=(20,)),
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    opt = tf.keras.optimizers.Adam(clipnorm=clip_norm)
    m.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])
    return m

# --- Train both models ---
drop_hist = dropout_model().fit(
    Xtr, ytr, epochs=20, batch_size=32,
    validation_data=(Xte, yte), verbose=0)

tf.keras.backend.clear_session()

clip_hist = clip_model().fit(
    Xtr, ytr, epochs=20, batch_size=32,
    validation_data=(Xte, yte), verbose=0)

# --- Combined Accuracy Plot (all curves on ONE graph) ---
plt.figure(figsize=(10, 6))

plt.plot(drop_hist.history['accuracy'],         '--', label='Dropout Train Acc')
plt.plot(drop_hist.history['val_accuracy'],      label='Dropout Val Acc')
plt.plot(clip_hist.history['accuracy'],         '--', label='GradClip Train Acc')
plt.plot(clip_hist.history['val_accuracy'],      label='GradClip Val Acc')

plt.title('Training & Validation Accuracy: Dropout vs Gradient Clipping')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

"""MULTITASKING AND EARLY STOPPING"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# --- Load & normalize MNIST ---
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

# --- Parity labels (sum-of-digits parity for single-digit labels == label % 2) ---
y_train_parity = y_train % 2
y_test_parity  = y_test  % 2

# --- Display a 5x5 sample of images with true labels ---
plt.figure(figsize=(6,6))
for i in range(25):
    plt.subplot(5,5,i+1); plt.axis('off')
    plt.imshow(x_train[i], cmap='gray'); plt.xlabel(y_train[i])
plt.show()

# --- Build multitask model: (10-class softmax) + (parity sigmoid) ---
inp = keras.Input(shape=(28,28))
x = layers.Flatten()(inp)
x = layers.Dense(128, activation='relu')(x)
out_class = layers.Dense(10, activation='softmax')(x)   # digit classification
out_par  = layers.Dense(1,  activation='sigmoid')(x)    # parity (binary)

model = keras.Model(inputs=inp, outputs=[out_class, out_par])

# --- Compile: separate losses for each head, track accuracies ---
model.compile(optimizer='adam',
              loss=['sparse_categorical_crossentropy', 'binary_crossentropy'],
              metrics=['accuracy','accuracy'])

# --- Early stopping callback (restore best weights) ---
es = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# --- Train (multitask: feed both targets) ---
hist = model.fit(x_train, [y_train, y_train_parity],
                 validation_split=0.2, epochs=20, callbacks=[es], verbose=1)

# --- Find epoch with minimum validation loss (1-based index) ---
early_epoch = np.argmin(hist.history['val_loss']) + 1
print("Early stopping occurred at epoch:", early_epoch)

# --- Plot total training & validation loss and mark early-stopping epoch ---
plt.plot(hist.history['loss'], label='train loss')
plt.plot(hist.history['val_loss'], label='val loss')
plt.axvline(early_epoch, color='r', linestyle='--', label='early stop')
plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(True); plt.show()