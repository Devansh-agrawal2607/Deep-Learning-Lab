# -*- coding: utf-8 -*-
"""LAB 1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mk1wQRrAP8Jvpzz8rJJNpCfGXH9DpK-O

Create and implement a basic neuron model within a computational framework.
Integrate essential elements like input nodes, weight parameters, bias, and an
activation function ( including but not limited to sigmoid, hyperbolic tangent, and
Rectified Linear Unit (ReLU) ) to construct a comprehensive representation of a
neuron. Evaluate and observe how each activation function influences the network's
behaviour and effectiveness in handling different types of data.
"""

import numpy as np
import matplotlib.pyplot as plt

# -------- Neuron Class --------
class Neuron:
    def __init__(self, input_size, seed=0):
        rng = np.random.default_rng(seed)
        self.w = rng.standard_normal(input_size)
        self.b = float(rng.standard_normal())

    def activate(self, z, activation):
        if activation == 'sigmoid':
            return 1 / (1 + np.exp(-z))
        if activation == 'tanh':
            return np.tanh(z)
        if activation == 'relu':
            return np.maximum(0, z)
        if activation == 'softmax':
            e = np.exp(z - np.max(z))
            return e / e.sum()
        raise ValueError("Choose: sigmoid, tanh, relu, softmax")

    def forward(self, x, activation='sigmoid'):
        z = x @ self.w + self.b
        return self.activate(z, activation)

# -------- Demo --------
input_size = 5
X = np.random.randn(10, input_size)
neuron = Neuron(input_size)

# Example forward passes (no stats printed)
sig_out  = neuron.forward(X[0], 'sigmoid')
tanh_out = neuron.forward(X[0], 'tanh')
relu_out = neuron.forward(X[0], 'relu')
softmax_out = neuron.activate(np.random.randn(5), 'softmax')


# -------- Plot Activation Curves --------
x = np.linspace(-6, 6, 400)

plt.figure(figsize=(12, 4))

plt.subplot(1, 4, 1)
plt.plot(x, 1/(1+np.exp(-x))); plt.title("Sigmoid"); plt.grid(True)

plt.subplot(1, 4, 2)
plt.plot(x, np.tanh(x)); plt.title("Tanh"); plt.grid(True)

plt.subplot(1, 4, 3)
plt.plot(x, np.maximum(0, x)); plt.title("ReLU"); plt.grid(True)

plt.subplot(1, 4, 4)
soft = np.exp(x) / np.sum(np.exp(x))
plt.plot(x, soft); plt.title("Softmax"); plt.grid(True)

plt.tight_layout()
plt.show()