# -*- coding: utf-8 -*-
"""LAB 8

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WX2m5Lyl10xIjxn8Do4qjzjX6lwGbzPl

Design a python program for Sentiment Analysis using Recurrent Neural Networks
(RNN).
"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

# --- Settings ---
max_features = 10000   # vocabulary size
maxlen = 200           # truncate/pad reviews to this length
embedding_dim = 50
batch_size = 128
epochs = 5

# --- Load & preprocess data ---
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)
x_train = pad_sequences(x_train, maxlen=maxlen)   # pad/truncate
x_test  = pad_sequences(x_test,  maxlen=maxlen)

# --- Build RNN model (Embedding -> LSTM -> Sigmoid) ---
model = Sequential([
    Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=maxlen),
    LSTM(64, dropout=0.2, recurrent_dropout=0.2),
    Dense(1, activation='sigmoid')   # binary sentiment
])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# --- Train ---
history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=1)

# --- Evaluate ---
loss, acc = model.evaluate(x_test, y_test, verbose=2)
print(f'Test accuracy: {acc:.4f}')

# --- Helper: predict sentiment for raw text (simple tokenization using imdb word index) ---
word_index = imdb.get_word_index()
def predict_sentiment(text):
    words = text.lower().split()
    seq = [word_index.get(w, 0) for w in words]                # map words -> indices (0 if OOV)
    seq = [i if i < max_features else 0 for i in seq]          # respect num_words limit
    padded = pad_sequences([seq], maxlen=maxlen)
    p = model.predict(padded, verbose=0)[0][0]
    return p  # probability ~1 positive, ~0 negative

# --- Example predictions ---
print("Positive example probability:", predict_sentiment("I loved this movie, it was fantastic and moving."))
print("Negative example probability:", predict_sentiment("Terrible film. I hated it and it was boring."))

# --- Plot training history (accuracy & loss) ---
plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='val')
plt.title('Accuracy'); plt.xlabel('Epoch'); plt.legend(); plt.grid(True)

plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='val')
plt.title('Loss'); plt.xlabel('Epoch'); plt.legend(); plt.grid(True)

plt.tight_layout(); plt.show()